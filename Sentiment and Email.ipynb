{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas for data frame\n",
    "#### the next 4 are for the classifiers\n",
    "#### CalibratedClassifierCV is to get probability for GaussianNB and LinearSVC\n",
    "#### CountVectorizer is to create a vector for words i.e. each word will be given a specific number\n",
    "#### pipeline has a function transformermixin, to create our own, we use this library\n",
    "#### Pipeline -> an input is given then the first function in the pipeline is run, the output of the first function becomes the input to the second function in the pipeline and so on\n",
    "#### spacy and stopwords\n",
    "#### punctuations\n",
    "#### for many files in a folder\n",
    "#### random to randomize sentiment data\n",
    "#### numpy for np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "from string import punctuation as punctuations\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from docx import Document\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enter path of training data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"C:/Users/jaski/OneDrive/Desktop/igt/ML_EMAIL6 - OTA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading english in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### default functions for spacy \n",
    "#### and spacy tokenizer is as we want to tokenize our sentences\n",
    "\n",
    "#### our tokenizer ignores all pronouns, proper nouns, stopwords and punctuations and converts string to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    tokens = nlp(sentence)\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in tokens if tok.pos_ != \"PROPN\" and tok.pos_ != \"PRON\"]\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]\n",
    "    return tokens\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the ml algorithms used are : LogisticRegression, Gaussian Naive Bayes, and Linear Support vector classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1 = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\")\n",
    "classifier2 = GaussianNB()\n",
    "classifier3 = LinearSVC()\n",
    "classifier4 = LogisticRegression(solver=\"lbfgs\")\n",
    "classifier2 = CalibratedClassifierCV(classifier2, cv=3)\n",
    "classifier3 = CalibratedClassifierCV(classifier3, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('classifier', classifier1)])\n",
    "pipe2 = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('to_dense', DenseTransformer()), \n",
    "                 ('classifier', classifier2)])\n",
    "pipe3 = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('to_dense', DenseTransformer()), \n",
    "                 ('classifier', classifier3)])\n",
    "\n",
    "\n",
    "pipe4 = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('classifier', classifier4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomizing our df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = df[:110]\n",
    "#test_df = df[110:]\n",
    "train_df = df\n",
    "train_X = train_df[\"text\"]\n",
    "train_y = train_df[\"label\"]\n",
    "#test_X = test_df[\"text\"]\n",
    "#test_y = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting all the data except sentiment\n",
    "## at one time, we can have only one set of data\n",
    "## so first we do all the task with email categorisation then we move on to sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x00000220BB0EA2B0>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "      ...enalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x00000220BB0EA278>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "      ...V(base_estimator=GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "            cv=3, method='sigmoid'))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x00000220BB0EA4A8>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "      ... penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "            cv=3, method='sigmoid'))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe3.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pipe1.score(test_X, test_y))\n",
    "#print(pipe2.score(test_X, test_y))\n",
    "#print(pipe3.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating a voted classifier using the 3 algorithms for the best result\n",
    "#### if 2 of the classifiers predict the same category, we take that category as the final output, probability becomes the mean of the two probabilities\n",
    "#### if all 3 same give different categories, we take the one with the maximum confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    data = [data]\n",
    "    pred = []\n",
    "    prob = []\n",
    "    pred.append(pipe1.predict(data))\n",
    "    pred.append(pipe2.predict(data))\n",
    "    pred.append(pipe3.predict(data))\n",
    "    prob.append(max(max(pipe1.predict_proba(data))))\n",
    "    prob.append(max(max(pipe2.predict_proba(data))))\n",
    "    prob.append(max(max(pipe3.predict_proba(data))))\n",
    "    for i in range(len(pred)):\n",
    "        for j in range(i+1,len(pred)):\n",
    "            if pred[i] == pred[j]:\n",
    "                return [' '.join(pred[i]), (prob[i] + prob[j])/2]\n",
    "    index = prob.index(max(prob))\n",
    "    return [' '.join(pred[index]), max(prob)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### path to folder containing all the email files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/jaski/OneDrive/Documents/python/one_many\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change the extension to get any other files\n",
    "#### will take only the files with that extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "filenames = []\n",
    "for filename in glob.glob(os.path.join(path, '*.docx')):\n",
    "    f = Document(filename)\n",
    "    filename = filename.split('\\\\')\n",
    "    filename = filename[1]\n",
    "    filenames.append(filename)\n",
    "    file = []\n",
    "    for para in f.paragraphs:\n",
    "        text += para.text\n",
    "        text += \"\\n\"\n",
    "        new_text = \"\"\n",
    "        for t in text:\n",
    "            if t == \"\\n\":\n",
    "                file.append(new_text)\n",
    "                new_text = \"\"\n",
    "            else:\n",
    "                new_text += t\n",
    "    file = [f for f in file if f != '']\n",
    "    files.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### path to positive, negative training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pos = open(\"C:/Users/jaski/OneDrive/Desktop/igt/own_positive.txt\", \"r\").read()\n",
    "short_neg = open(\"C:/Users/jaski/OneDrive/Desktop/igt/own_negative.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### manipulating the sentiment data and shuffling it to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx = []\n",
    "for r in short_pos.split(\"\\n\"):\n",
    "    docx.append((r, \"pos\"))\n",
    "for r in short_neg.split(\"\\n\"):\n",
    "    docx.append((r, \"neg\"))\n",
    "random.shuffle(docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sentiment = docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_pred_category -> [ category of para1, para2, para3 ]\n",
    "#### df_prob_category -> [ probability of para1, para2, para3 ]\n",
    "#### df_pred_sentiment -> [ pos, neut, neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_category = []\n",
    "df_prob_category = []\n",
    "df_pred_sentiment = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### storing categories for each para\n",
    "#### we sort before appending np.nan because sorting after will give random results\n",
    "#### np.nan will give blank in csv or excel\n",
    "#### len(file, 3) will work only if len of file is less than 3 \n",
    "#### if it is greater than 3, it will not go in the loop\n",
    "#### in the end, we take the first 3 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    local_pred = []\n",
    "    local_prob = []\n",
    "    for para in file:\n",
    "        predprob = predict(para)\n",
    "        local_pred.append(predprob[0])\n",
    "        local_prob.append(predprob[1])\n",
    "    local_prob, local_pred = zip(*sorted(zip(local_prob, local_pred)))\n",
    "    for i in range(len(file),3):\n",
    "        local_pred.append(np.nan)\n",
    "        local_prob.append(np.nan)\n",
    "    df_pred_category.append(local_pred[:3])\n",
    "    df_prob_category.append(local_prob[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our tasks with email categorization finish and we move on to sentiment\n",
    "\n",
    "#### fitting sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x00000220BB0EA470>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "      ...enalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe4.fit([x[0] for x in train_data_sentiment], [x[1] for x in train_data_sentiment])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### probability < 0.65 will be neutral\n",
    "#### array -> [ pos, neut, neg ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    test_sentiment_data = ' '.join(file)\n",
    "    local_pred = pipe4.predict([test_sentiment_data])\n",
    "    local_prob = max(max(pipe4.predict_proba([test_sentiment_data])))\n",
    "    if local_prob < 0.65:\n",
    "        df_pred_sentiment.append([np.nan, \"neut\", np.nan])\n",
    "    elif local_pred == \"pos\":\n",
    "        df_pred_sentiment.append([local_prob, np.nan, np.nan])\n",
    "    else:\n",
    "        df_pred_sentiment.append([np.nan, np.nan, local_prob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating dictionary to create the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\"ID\":filenames,\n",
    "          \"Category 1\": [df[0] for df in df_pred_category], \"Prob 1\": [df[0] for df in df_prob_category],\n",
    "          \"Category 2\": [df[1] for df in df_pred_category], \"Prob 2\": [df[1] for df in df_prob_category],\n",
    "          \"category 3\": [df[2] for df in df_pred_category], \"Prob 3\": [df[2] for df in df_prob_category],\n",
    "          \"Positive Feedback\": [df[0] for df in df_pred_sentiment],\n",
    "          \"Neutral Feedback\": [df[1] for df in df_pred_sentiment],\n",
    "          \"Negative Feedback\": [df[2] for df in df_pred_sentiment]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating df then to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"C:/Users/jaski/OneDrive/Desktop/igt/email_para_cat_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
